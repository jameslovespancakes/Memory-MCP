<div align="center">

# ğŸ§  Hippocampus Memory MCP Server

### *Persistent, Semantic Memory for Large Language Models*

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![MCP](https://img.shields.io/badge/MCP-1.2.0-green.svg)](https://modelcontextprotocol.io/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Architecture](#-architecture)

---

</div>

## ğŸ“– Overview

A Python-based **Model Context Protocol (MCP)** server that gives LLMs persistent, hippocampus-inspired memory across sessions. Store, retrieve, consolidate, and forget memories using semantic similarity search powered by vector embeddings.

**Why Hippocampus?** Just like the human brain's hippocampus consolidates short-term memories into long-term storage, this server intelligently manages LLM memory through biological patterns:
- ğŸ”„ **Consolidation** - Merge similar memories to reduce redundancy
- ğŸ§¹ **Forgetting** - Remove outdated information based on age/importance
- ğŸ” **Semantic Retrieval** - Find relevant memories through meaning, not keywords

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ—„ï¸ **Vector Storage** | FAISS-powered semantic similarity search |
| ğŸ¯ **MCP Compliant** | Full MCP 1.2.0 spec compliance via FastMCP |
| ğŸ§¬ **Bio-Inspired** | Hippocampus-style consolidation and forgetting |
| ğŸ”’ **Security** | Input validation, rate limiting, injection prevention |
| ğŸ” **Semantic Search** | Sentence transformer embeddings (CPU-optimized) |
| â™¾ï¸ **Unlimited Storage** | No memory count limits, only per-item size limits |
| ğŸ†“ **100% Free** | Local embedding model - no API costs |

## ğŸš€ Quick Start

### 5 Core MCP Tools

```python
memory_read         # ğŸ” Retrieve memories by semantic similarity
memory_write        # âœï¸  Store new memories with tags & metadata
memory_consolidate  # ğŸ”„ Merge similar memories
memory_forget       # ğŸ§¹ Remove memories by age/importance/tags
memory_stats        # ğŸ“Š Get system statistics
```

## ğŸ“¦ Installation

### Prerequisites

- Python 3.9+
- ~200MB disk space (for embedding model)

### Setup in 3 Steps

```bash
# 1. Clone the repository
git clone https://github.com/jameslovespancakes/Memory-MCP.git
cd Memory-MCP

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run the server
python -m memory_mcp_server.server
```

### Claude Desktop Integration

Add to your Claude Desktop config (`claude_desktop_config.json`):

```json
{
  "mcpServers": {
    "memory": {
      "command": "python",
      "args": ["-m", "memory_mcp_server.server"],
      "cwd": "/path/to/Memory-MCP"
    }
  }
}
```

> **ğŸ‰ That's it!** Claude will now have persistent memory across conversations.

## ğŸ“š Documentation

### Memory Operations via MCP

Once connected to Claude, use natural language:

```
"Remember that I prefer Python for backend development"
â†’ Claude calls memory_write()

"What do you know about my programming preferences?"
â†’ Claude calls memory_read()

"Consolidate similar memories to clean up storage"
â†’ Claude calls memory_consolidate()
```

### Direct API Usage

#### âœï¸ Writing Memories

```python
from memory_mcp_server.storage import MemoryStorage
from memory_mcp_server.tools import MemoryTools

storage = MemoryStorage(storage_path="my_memory")
await storage._ensure_initialized()
tools = MemoryTools(storage)

# Store with tags and importance
await tools.memory_write(
    text="User prefers dark mode UI",
    tags=["preference", "ui"],
    importance_score=3.0,
    metadata={"category": "settings"}
)
```

#### ğŸ” Reading Memories

```python
# Semantic search
result = await tools.memory_read(
    query_text="What are my UI preferences?",
    top_k=5,
    min_similarity=0.3
)

# Filter by tags and date
result = await tools.memory_read(
    query_text="Python learning",
    tags=["learning", "python"],
    date_range_start="2024-01-01"
)
```

#### ğŸ”„ Consolidating Memories

```python
# Merge similar memories (threshold: 0.85)
result = await tools.memory_consolidate(similarity_threshold=0.85)
print(f"Merged {result['consolidated_groups']} groups")
```

#### ğŸ§¹ Forgetting Memories

```python
# Remove by age
await tools.memory_forget(max_age_days=30)

# Remove by importance
await tools.memory_forget(min_importance_score=2.0)

# Remove by tags
await tools.memory_forget(tags_to_forget=["temporary"])
```

### Testing

Run the included test suite:

```bash
python test_memory.py
```

This tests all 5 operations with sample data.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MCP Client (Claude Desktop, etc.)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ JSON-RPC over stdio
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastMCP Server (server.py)                         â”‚
â”‚  â”œâ”€ memory_read                                     â”‚
â”‚  â”œâ”€ memory_write                                    â”‚
â”‚  â”œâ”€ memory_consolidate                              â”‚
â”‚  â”œâ”€ memory_forget                                   â”‚
â”‚  â””â”€ memory_stats                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Memory Tools (tools.py)                            â”‚
â”‚  â”œâ”€ Input validation & sanitization                â”‚
â”‚  â””â”€ Rate limiting (100 req/min)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Storage Layer (storage.py)                         â”‚
â”‚  â”œâ”€ Sentence Transformers (all-MiniLM-L6-v2)       â”‚
â”‚  â”œâ”€ FAISS Vector Index (cosine similarity)         â”‚
â”‚  â””â”€ JSON persistence (memories.json)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Memory Lifecycle

| Step | Process | Technology |
|------|---------|------------|
| ğŸ“ **Write** | Text â†’ 384-dim vector embedding | Sentence Transformers (CPU) |
| ğŸ’¾ **Store** | Normalized vector â†’ FAISS index | FAISS IndexFlatIP |
| ğŸ” **Search** | Query â†’ embedding â†’ top-k similar | Cosine similarity |
| ğŸ”„ **Consolidate** | Group similar (>0.85) â†’ merge | Vector clustering |
| ğŸ§¹ **Forget** | Filter by age/importance/tags â†’ delete | Metadata filtering |

## ğŸ”’ Security

| Protection | Implementation |
|------------|----------------|
| ğŸ›¡ï¸ **Injection Prevention** | Regex filtering of script tags, eval(), path traversal |
| â±ï¸ **Rate Limiting** | 100 requests per 60-second window per client |
| ğŸ“ **Size Limits** | 50KB text, 5KB metadata, 20 tags per memory |
| âœ… **Input Validation** | Pydantic models + custom sanitization |
| ğŸ” **Safe Logging** | stderr only (prevents JSON-RPC corruption) |

## âš™ï¸ Configuration

### Environment Variables

```bash
MEMORY_STORAGE_PATH="memory_data"           # Storage directory
EMBEDDING_MODEL="all-MiniLM-L6-v2"          # Model name
RATE_LIMIT_REQUESTS=100                     # Max requests
RATE_LIMIT_WINDOW=60                        # Time window (seconds)
```

### Storage Limits

- âœ… **Unlimited total memories** (no count limit)
- âš ï¸ Per-memory limits: 50KB text, 5KB metadata, 20 tags

## ğŸ› Troubleshooting

<details>
<summary><b>Model won't download</b></summary>

First run downloads `all-MiniLM-L6-v2` (~90MB). Ensure internet connection and `~/.cache/` write permissions.
</details>

<details>
<summary><b>PyTorch compatibility errors</b></summary>

```bash
pip uninstall torch transformers sentence-transformers -y
pip install torch==2.1.0 transformers==4.35.2 sentence-transformers==2.2.2
```
</details>

<details>
<summary><b>Memory errors on large operations</b></summary>

The model runs on CPU. Ensure 2GB+ free RAM. Reduce `top_k` in read operations if needed.
</details>

## ğŸ“ License

MIT License - feel free to use in your projects!

## ğŸ¤ Contributing

PRs welcome! Please:
- Follow MCP security guidelines
- Add tests for new features
- Update documentation

## ğŸ”— Resources

- [Model Context Protocol Docs](https://modelcontextprotocol.io/)
- [FastMCP Framework](https://github.com/jlowin/fastmcp)
- [FAISS Documentation](https://github.com/facebookresearch/faiss)
- [Sentence Transformers](https://www.sbert.net/)

---

<div align="center">

**Built with ğŸ§  for persistent LLM memory**

[Report Bug](https://github.com/jameslovespancakes/Memory-MCP/issues) Â· [Request Feature](https://github.com/jameslovespancakes/Memory-MCP/issues)

</div>